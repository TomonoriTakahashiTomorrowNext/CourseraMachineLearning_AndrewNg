{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week3 分類問題（Classification problem）\n",
    "## 目的：新しいデータに対して、それがどの分類に入るかを予測したい。\n",
    "$x$ : 与えられたデータ\n",
    "\n",
    "$y$ : $x$からの予測\n",
    "### 例：10秒後にドル円相場は上がるか？下がるか？　を過去3分間1秒ごとのMidから予測したい\n",
    "\n",
    "学習に必要なデータ\n",
    "\n",
    "- $x_0^{(0)}, \\cdots, x_{180}^{(0)}$ : 2017.04.01T00:00:00～2017.04.01T00:03:00の1秒ごとのドル円レート\n",
    "- $x_0^{(1)}, \\cdots, x_{180}^{(1)}$ : 2017.04.01T00:00:01～2017.04.01T00:03:01の1秒ごとのドル円レート\n",
    "- $\\cdots$\n",
    "- $x_0^{(k)}, \\cdots, x_{180}^{(k)}$ : 2017.05.10T12:34:56～2017.05.10T12:37:56の1秒ごとのドル円レート\n",
    "- $\\cdots$\n",
    "- $x_0^{(m)}, \\cdots, x_{180}^{(m)}$ : 2017.05.26T23:57:00～2017.05.26T24:00:00の1秒ごとのドル円レート\n",
    "\n",
    "\n",
    "- $y^{(0)}$ : 2017.04.01T00:03:00から10秒後にマーケットが上がったら1、下がったら0\n",
    "- $\\cdots$\n",
    "- $y^{(m)}$ : 2017.05.27T00:00:00から10秒後にマーケットが上がったら1、下がったら0\n",
    "\n",
    "今から予測したいこと\n",
    "- $x_0, x_1, \\cdots, x_{180}$ : 3分前～現在の1秒おきのドル円レート\n",
    "- $y=0 今から10秒後に下がるなら0、上がるなら1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目標：新しいデータ$x$が与えられたときに、$y=1$である「確率」を知りたい\n",
    "\n",
    "- パラメータ$\\theta$、新しいデータ$x$が与えられたときに、$y=1$である確率を表す式$h_\\theta (x)$を次のように立てる。\n",
    "- $h_\\theta (x) = g(\\theta^T x)$\n",
    "- $g(z) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$\\theta^T X$は前回やったのと同じだが、$\\theta^T X$のままだと$-\\infty$から$+\\infty$までの値をとりうるため、0~1の間に値をとるために$g(z) = \\frac{1}{1 + e^{-x}}$をかませる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d5010adba8>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFkCAYAAACw3EhvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d563bcac18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = 0.5 * np.sort(np.r_[np.arange(11), - np.arange(11)])\n",
    "y1 = 1 / (1 + np.exp(-x1))\n",
    "plt.plot(x1, y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のグラフからわかるように、\n",
    "- $g(z)$は0を中心として対称。\n",
    "- $z>0$のとき$g(z)>\\frac{1}{2}$。$z<0$のとき$g(z)<\\frac{1}{2}$。$z=0$のとき$g(z)=0.5$\n",
    "\n",
    "### これからやること\n",
    "\n",
    "- ①学習データ$\\{x_i^{(j)}\\}_{(i, j)}$からパラメータ$\\theta$を推計する。\n",
    "- ②新しいデータ$x=(x_0, \\cdots, x_n)$が与えられたときに、学習済みパラメータ$\\theta$を用いて\n",
    "$y = h_\\theta (x) = g(\\theta^T x) = \\frac{1}{1 + e^{- \\theta^T x}}$を計算する。\n",
    "- ③$y > 0$か$y < 0$かどうかを判断する。\n",
    "\n",
    "一番難しい（かつ、意味のある）のが①なので、このやり方を深掘り。\n",
    "\n",
    "### 前回（LinearRegression）はどうやったか\n",
    "- コスト関数$J(\\theta) = \\frac{1}{m} \\sum_{i = 1}^m Cost(x^{(i)}, y^{(i)})$を用意する。\n",
    "- Gradient descentでコスト関数$J(\\theta)$を最小にするパラメータ$\\theta$を求める。\n",
    "\n",
    "### 今回はどうするか\n",
    "基本戦略は同じ。ただし、コスト関数は次のように定義する。\n",
    "\n",
    "- $J(\\theta) = \\frac{1}{m} \\sum_{i = 1}^m Cost(h_\\theta (x^{(i)}), y^{(i)})$\n",
    "\n",
    "- $Cost(h_\\theta (x)), y) = - log (h_\\theta (x))$ if $y = 1$\n",
    "\n",
    "- $Cost(h_\\theta (x)), y) = - log (1 - h_\\theta (x))$ if $y = 0$\n",
    "\n",
    "さらに、上のように書くと面倒なので、一つの式にまとめると次のようになる。\n",
    "\n",
    "- $J(\\theta) = - \\frac{1}{m} \\sum_{i = 1}^m \\{ y^{(i)} log (h_\\theta (x^{(i)})) + (1 - y^{(i)}) log (1 - h_\\theta (x^{(i)})) \\}$\n",
    "\n",
    "Gradient Descentも同じ。今回はコスト関数の微分は次式。\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta _j} = \\frac{1}{m} \\sum_{i = 1}^m (h_\\theta (x^{(i)}) - y^{(i)}) x_j^{(i)}$\n",
    "\n",
    "あとは同じように逐次推計値$\\theta_j$を更新していけばよい。\n",
    "\n",
    "$\\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^{m} ((h_\\theta (x^{(i)}) - y^{(i)}) x_j^{(i)}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多値分類問題\n",
    "$y=0$ or $1$ だけでなく、一般的にはもっと多くの分類をしたい。\n",
    "- 10秒後のドル円相場は　上がる（$y=1$）、下がる（$y=-1$）、変わらない（$y=0$）\n",
    "- 10秒後のドル円相場は　10銭以上上がる、5銭～10銭上がる、5銭以内で上がる、5銭以内で下がる、5～10銭下がる、10銭以上下がる\n",
    "\n",
    "### One-vs-All\n",
    "$h_\\theta (x) = g(\\theta^T x)$は$y=1$である確率を表すのだった。\n",
    "\n",
    "これを拡張して、$h_\\theta^{(0)} (x)$は$y=0$である確率を、$h_\\theta^{(1)} (x)$は$y=1$である確率を、$h_\\theta^{(2)} (x)$は$y=2$である確率を、それぞれ表すようにする。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done.\n",
      "(5000, 400)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# data load OLD Version\n",
    "import csv\n",
    "f = open('week3_y.csv', 'r')\n",
    "dataReader = csv.reader(f)\n",
    "y = np.zeros(5000)\n",
    "i = 0\n",
    "for row in dataReader:\n",
    "    for c in row:\n",
    "        y[i] = float(c) # float(c) : 文字列cを実数値型に変換する\n",
    "    i = i + 1\n",
    "\n",
    "f = open('week3_X.csv', 'r')\n",
    "dataReader = csv.reader(f)\n",
    "X = np.zeros(5000 * 400).reshape(5000, 400)\n",
    "i = 0\n",
    "for row in dataReader:\n",
    "    j = 0\n",
    "    for c in row:\n",
    "        X[i, j] = float(c)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "\n",
    "print('load done.')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done.\n",
      "(5000, 400)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "# data load new version\n",
    "import pandas as pd\n",
    "X = pd.read_csv(\"week3_X.csv\", header=None).values\n",
    "y = pd.read_csv(\"week3_y.csv\", header=None).values\n",
    "print(\"load done.\")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- コスト関数$J(\\theta)$を定義\n",
    "\n",
    "以後、標本数を$m$、説明変数の数を$n$で表す。\n",
    "\n",
    "\\begin{eqnarray}\n",
    "J(\\theta) &=& \\frac{1}{m} \\sum_{i=1}^m \\{ - y^{(i)} \\log (h_\\theta (x^{(i)})) - (1 - y^{(i)}) \\log (1 - h_\\theta (x^{(i)}) ) \\} \\\\\n",
    "&=& \\frac{1}{m} \\sum_{i=1}^m \\{ - y^{(i)} \\log (g(\\theta^T x^{(i)})) - (1 - y^{(i)}) \\log (1 - g(\\theta^T x^{(i)})) \\} \\\\\n",
    "&=& \\frac{1}{m} \\sum_{i=1}^m \\{ - y^{(i)} \\log (g(x^{(i) T} \\theta)) - (1 - y^{(i)}) \\log (1 - g( x^{(i) T} \\theta)) \\}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    grad J = \n",
    "    \\begin{bmatrix}\n",
    "        \\frac{\\partial J}{\\partial \\theta_0} \\\\\n",
    "        \\frac{\\partial J}{\\partial \\theta_1} \\\\\n",
    "        \\cdots \\\\\n",
    "        \\frac{\\partial J}{\\partial \\theta_n} \\\\\n",
    "    \\end{bmatrix}\n",
    "    &=& \\frac{1}{m}\n",
    "    \\begin{bmatrix}\n",
    "        \\sum_{i=1}^m ((h_\\theta (x^{(i)}) - y^{(i)}) x_0^{(i)}) \\\\\n",
    "        \\sum_{i=1}^m ((h_\\theta (x^{(i)}) - y^{(i)}) x_1^{(i)}) \\\\\n",
    "        \\cdots \\\\\n",
    "        \\sum_{i=1}^m ((h_\\theta (x^{(i)}) - y^{(i)}) x_n^{(i)})\n",
    "    \\end{bmatrix} \\\\\n",
    "    &=& \\frac{1}{m} \\sum_{i = 1}^m ((h_\\theta (x^{(i)}) - y^{(i)}) x^{(i)}) \\\\\n",
    "    &=& \\frac{1}{m} X^T\n",
    "        \\begin{bmatrix}\n",
    "            h_\\theta (x^{(1)}) - y^{(1)} \\\\\n",
    "            h_\\theta (x^{(2)}) - y^{(2)} \\\\\n",
    "            \\cdots \\\\\n",
    "            h_\\theta (x^{(m)}) - y^{(m)} \\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[ 0.26894142  0.5         0.73105858]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(- z))\n",
    "\n",
    "# Test\n",
    "print(sigmoid(0.0))\n",
    "print(sigmoid(np.array([-1.0, 0.0, 1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[ 1.  2.  3.]\n",
      "[ 0.  2.  4.]\n",
      "[-0. -1. -2.]\n",
      "[ 1.          2.71828183  7.3890561 ]\n",
      "[ 1.  2.]\n",
      "----\n",
      "(2, 3)\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[  5.  14.]\n",
      "----\n",
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "a = np.array([0.0, 1.0, 2.0])\n",
    "print(a.shape)\n",
    "print(1 + a) # 配列の全ての要素に1を足す\n",
    "print(2 * a) # 配列の全ての要素に2を掛ける\n",
    "print(- a) # 配列の全ての要素の符号を反転\n",
    "print(np.exp(a)) # 配列の全ての要素のexpをとる\n",
    "print(a[1:]) # 配列の1番目以降からの要素を抜き取る（スライシング）\n",
    "print(\"----\")\n",
    "b = np.arange(6).reshape(2, 3) # 2x3行列\n",
    "print(b.shape)\n",
    "print(b)\n",
    "print(b.dot(a)) # 行列演算として（2x3行列）と（3x1行列）を掛ける。自動的に配列を3x1行列とみなすらしい\n",
    "print(\"----\")\n",
    "print((a == 1) + 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 5000)\n",
      "(400,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (399,5000) (399,) (399,5000) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2cdb5b36664e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlrCostFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlrCostFunctionGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-2cdb5b36664e>\u001b[0m in \u001b[0;36mlrCostFunctionGrad\u001b[0;34m(theta, X, y, _lambda)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_lambda\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Regularized term\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (399,5000) (399,) (399,5000) "
     ]
    }
   ],
   "source": [
    "def lrCostFunction(theta, X, y, _lambda):\n",
    "    # コスト関数の本体\n",
    "    h_theta_X = sigmoid(X.dot(theta)) # (\\theta^T x^{(i)})_i\n",
    "    J = np.sum(- y * np.log(h_theta_X) - (1 - y) * np.log(1.0 - h_theta_X))\n",
    "    m = X.shape[0]\n",
    "    J /= m\n",
    "    # Regularized term\n",
    "    J += 0.5 * (_lambda / m) * np.sum(np.square(theta[1:])) # 0番目要素は入れない\n",
    "    return J\n",
    "\n",
    "def lrCostFunctionGrad(theta, X, y, _lambda):\n",
    "    h_theta_X = sigmoid(X.dot(theta))\n",
    "    m = X.shape[0]\n",
    "    grad = (X.T).dot(h_theta_X - y) / m\n",
    "    print(grad.shape)\n",
    "    print(theta.shape)\n",
    "    grad[1:] += (_lambda / m) * theta[1:] # Regularized term\n",
    "    return grad\n",
    "\n",
    "# Test\n",
    "theta = np.zeros(X.shape[1])\n",
    "J = lrCostFunction(theta, X, y, 0.0)\n",
    "grad = lrCostFunctionGrad(theta, X, y, 0.0)\n",
    "print(grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as spo\n",
    "import pandas as pd\n",
    "def lrCostFunction_Wrapper(param, *args):\n",
    "    \"\"\"lrCostFunctionをfmin_cgに通すためのラッパー関数\"\"\"\n",
    "    theta = param\n",
    "    X, y, _lambda = args\n",
    "    return lrCostFunction(theta, X, y, _lambda)\n",
    "\n",
    "def lrCostFunctionGrad_Wrapper(param, *args):\n",
    "    \"\"\"lrCostFunctionGradをfmin_cgに通すためのラッパー関数\"\"\"\n",
    "    theta = param\n",
    "    X, y, _lambda = args\n",
    "    return lrCostFunctionGrad(theta, X, y, _lambda)\n",
    "\n",
    "def oneVsAll(X, y, num_labels, _lambda):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    allTheta = np.zeros((num_labels, n + 1)) # 求めるべき\\theta 0~9までの10個分、各400+1個。（+1個はBias項）\n",
    "    X = np.concatenate((np.ones((m, 1)), X), axis = 1) # XにBias項相当のデータ(-1)を追加\n",
    "    for i in range(0, num_labels):\n",
    "        initialTheta = np.ones((n + 1, 1)) # 当初は初期値0でやっていたものの、収束がよくないので初期値1に変更。\n",
    "        y_zero_or_one = (y == i) + 0\n",
    "        # np.savetxt(\"y_\" + str(i) + \".csv\", y_zero_or_one, delimiter=\"\\n\")\n",
    "        args = X, y_zero_or_one, _lambda\n",
    "        resultTheta = spo.fmin_cg(lrCostFunction_Wrapper, initialTheta, fprime=lrCostFunctionGrad_Wrapper, args=args)\n",
    "        allTheta[i, :] = resultTheta\n",
    "    return allTheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.008570\n",
      "         Iterations: 133\n",
      "         Function evaluations: 535\n",
      "         Gradient evaluations: 535\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.013138\n",
      "         Iterations: 140\n",
      "         Function evaluations: 535\n",
      "         Gradient evaluations: 535\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.050833\n",
      "         Iterations: 230\n",
      "         Function evaluations: 729\n",
      "         Gradient evaluations: 729\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.057612\n",
      "         Iterations: 261\n",
      "         Function evaluations: 806\n",
      "         Gradient evaluations: 806\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.033093\n",
      "         Iterations: 207\n",
      "         Function evaluations: 688\n",
      "         Gradient evaluations: 688\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.054519\n",
      "         Iterations: 236\n",
      "         Function evaluations: 725\n",
      "         Gradient evaluations: 725\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.018262\n",
      "         Iterations: 174\n",
      "         Function evaluations: 656\n",
      "         Gradient evaluations: 656\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.030648\n",
      "         Iterations: 211\n",
      "         Function evaluations: 707\n",
      "         Gradient evaluations: 707\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078455\n",
      "         Iterations: 331\n",
      "         Function evaluations: 943\n",
      "         Gradient evaluations: 943\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.071198\n",
      "         Iterations: 294\n",
      "         Function evaluations: 867\n",
      "         Gradient evaluations: 867\n",
      "Optimization was done.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "_lambda = 0.1\n",
    "allTheta = oneVsAll(X, y, 10, _lambda)\n",
    "print('Optimization was done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9646\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "def predictOneVsAll(allTheta, X):\n",
    "    m = X.shape[0]\n",
    "    X = np.concatenate((np.ones((m, 1)), X), axis = 1)\n",
    "    prob = sigmoid(X.dot(allTheta.T))\n",
    "    res = prob.argmax(axis = 1)\n",
    "    return res\n",
    "\n",
    "predicted = predictOneVsAll(allTheta, X)\n",
    "np.savetxt(\"predicted.csv\", predicted, delimiter=\"\\n\")\n",
    "print(((predicted == y) + 0).mean())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
